{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adaptive-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "import operator\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "continental-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, data = None, next_node = None, prev_node = None):\n",
    "        self.data = data\n",
    "        self.next_node =  next_node\n",
    "        self.prev_node = prev_node\n",
    "\n",
    "class DoublyLinkedList(object):\n",
    "    def __init__(self, head = None):\n",
    "        self.head = head\n",
    "    \n",
    "    def traverse(self):\n",
    "        current_node = self.head\n",
    "        while current_node != None:\n",
    "            print(current_node.data)\n",
    "            current_node = current_node.next_node\n",
    "    \n",
    "    def get_size(self):\n",
    "        count = 0\n",
    "        current_node = self.head\n",
    "        while current_node != None:\n",
    "            count += 1\n",
    "            current_node = current_node.next_node\n",
    "        return count\n",
    "            \n",
    "    def append(self, data):\n",
    "        new_node = Node(data)\n",
    "        current_node = self.head\n",
    "        new_node.next_node = current_node\n",
    "        new_node.prev_node = None\n",
    "        if current_node != None:\n",
    "            current_node.prev_node = new_node\n",
    "        self.head = new_node\n",
    "      \n",
    "    def delete(self, data):\n",
    "        current_node = self.head\n",
    "        while current_node != None:\n",
    "            if current_node.data == data and current_node == self.head:\n",
    "                if not current_node.next_node:\n",
    "                    current_node = None\n",
    "                    self.head = None\n",
    "                    return\n",
    "                else:\n",
    "                    q = current_node.next_node\n",
    "                    current_node.next_node = None\n",
    "                    q.prev_node = None\n",
    "                    current_node = None\n",
    "                    self.head = q\n",
    "                    return\n",
    "            \n",
    "            elif current_node.data == data:\n",
    "                if current_node.next_node != None:\n",
    "                    p = current_node.prev_node\n",
    "                    q = current_node.next_node\n",
    "                    p.next_node = q\n",
    "                    q.prev_node = p\n",
    "                    current_node.next_node = None\n",
    "                    current_node.prev_node = None\n",
    "                    current_node = None\n",
    "                    return\n",
    "                else:\n",
    "                    p = current_node.prev_node\n",
    "                    p.next_node = None\n",
    "                    current_node.prev_node = None\n",
    "                    current = None\n",
    "                    return\n",
    "            current_node =  current_node.next_node\n",
    "\n",
    "class Bucket(object):\n",
    "    def __init__(self, value):\n",
    "        self.gain = value\n",
    "        self.data = DoublyLinkedList()\n",
    "\n",
    "class Bucket_Arrays(object):\n",
    "    def __init__(self, maxdegree):\n",
    "        self.bucket_range = maxdegree\n",
    "        self.left_buckets = np.array([Bucket(i) for i in range(-self.bucket_range,self.bucket_range+1)])\n",
    "        self.right_buckets = np.array([Bucket(i) for i in range(-self.bucket_range,self.bucket_range+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "confidential-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_num_cuts(df):\n",
    "    connected_edges = df[df['Partition']==0]['ID connected vertices'].values\n",
    "    outside_partition = list(df[df['Partition']==1].index)\n",
    "    connected_edges = [item in outside_partition for sublist in connected_edges for item in sublist]\n",
    "    num_cuts = np.sum(connected_edges)\n",
    "    return num_cuts\n",
    "\n",
    "def initialize_gain_buckets(df, buckets):\n",
    "    max_degree = np.max(df['Number of connected vertices'])\n",
    "    for i in range(len(df)):\n",
    "        connected_edges = df['ID connected vertices'][i]\n",
    "        partition_value = df['Partition'][i]\n",
    "        gain = 0\n",
    "        for j in connected_edges:\n",
    "            connected_partition = df['Partition'][j]\n",
    "            if(partition_value != connected_partition):\n",
    "                gain += 1\n",
    "            else:\n",
    "                gain -= 1\n",
    "        df.loc[i,'Gain'] = gain\n",
    "        \n",
    "        if partition_value == 0:\n",
    "            buckets.left_buckets[gain+max_degree].data.append(i)\n",
    "\n",
    "        else:\n",
    "            buckets.right_buckets[gain+max_degree].data.append(i)\n",
    "    \n",
    "    return df,buckets\n",
    "\n",
    "def calculate_vertex_max_gain(df,bucket_choice,max_degree):\n",
    "    if(bucket_choice == 0):\n",
    "        vertex_max_gain = df[df['Partition']==0]['Gain'].idxmax()\n",
    "    else:\n",
    "        vertex_max_gain = df[df['Partition']==1]['Gain'].idxmax()\n",
    "    return vertex_max_gain\n",
    "\n",
    "def update_df_buckets(df,buckets,vertex_max_gain):\n",
    "    max_degree = np.max(df['Number of connected vertices'])\n",
    "    partition_value = df.loc[vertex_max_gain,'Partition']\n",
    "    df.loc[vertex_max_gain,'Partition'] = int(not(partition_value))\n",
    "    if df.loc[vertex_max_gain,'Partition'] == 1:\n",
    "        gain = df.loc[vertex_max_gain,'Gain']\n",
    "        buckets.left_buckets[gain+max_degree].data.delete(vertex_max_gain)\n",
    "        df,buckets = re_calculate_gain(df, buckets,vertex_max_gain)\n",
    "        \n",
    "    else:\n",
    "        gain = df.loc[vertex_max_gain,'Gain']\n",
    "        buckets.right_buckets[gain+max_degree].data.delete(vertex_max_gain)\n",
    "        df,buckets = re_calculate_gain(df, buckets,vertex_max_gain)\n",
    "    return df, buckets\n",
    "\n",
    "def re_calculate_gain(df, buckets, vertex_max_gain):\n",
    "    max_degree = np.max(df['Number of connected vertices'])\n",
    "    vertices_to_update = df.loc[vertex_max_gain]['ID connected vertices']\n",
    "    \n",
    "    changed_partition = df.iloc[vertex_max_gain]['Partition']\n",
    "    for i in vertices_to_update:\n",
    "        if(df.loc[i]['Fixed'] == 0):\n",
    "            new_gain = 0\n",
    "            current_gain = df.loc[i]['Gain']\n",
    "            partition_value = df.iloc[i]['Partition']\n",
    "            if(partition_value == changed_partition):\n",
    "                new_gain = current_gain+2\n",
    "            else:\n",
    "                new_gain = current_gain-2\n",
    "            \n",
    "            df.loc[i,'Gain'] = new_gain\n",
    "            if(partition_value == 1): \n",
    "                buckets.right_buckets[new_gain+max_degree].data.append(i)\n",
    "            else:\n",
    "                buckets.left_buckets[new_gain+max_degree].data.append(i)\n",
    "    df.loc[vertex_max_gain,'Fixed'] = 1\n",
    "    df.loc[vertex_max_gain,'Gain'] = -999\n",
    "    return df, buckets\n",
    "\n",
    "def initialise_data(partitioning):\n",
    "    # Loading the single planar graph of 500 vertices\n",
    "    data = defaultdict(list)\n",
    "    for line in open(\"Graph500.txt\"):\n",
    "        split_line=line.split()\n",
    "        ID_vertex = int(split_line[0])\n",
    "        num_connected_vertices  = int(split_line[2])\n",
    "        ID_connected_vertices = [int(i)-1 for i in split_line[3:]]\n",
    "        if (ID_vertex) not in data.keys():\n",
    "            data[ID_vertex].append(int(num_connected_vertices))\n",
    "            data[ID_vertex].append(0)\n",
    "            data[ID_vertex].append(0)\n",
    "            data[ID_vertex].append(ID_connected_vertices)\n",
    "            data[ID_vertex].append(0)\n",
    "    data_frame = pd.DataFrame(data.values(),columns = ['Number of connected vertices','Gain', 'Fixed','ID connected vertices', 'Partition'])\n",
    "    \n",
    "    num_vertices = len(data_frame)\n",
    "    if(partitioning is None):\n",
    "        partition = random.sample(range(0,num_vertices),num_vertices//2)\n",
    "        data_frame.loc[partition,'Partition'] = 1\n",
    "    else:\n",
    "        data_frame[\"Partition\"] = partitioning\n",
    "    return data_frame\n",
    "\n",
    "def FM_one_pass(df):\n",
    "    num_cuts = calculate_num_cuts(df)\n",
    "    num_vertices = len(df)\n",
    "    min_cuts = num_cuts\n",
    "    max_degree = np.max(df['Number of connected vertices'])\n",
    "    buckets = Bucket_Arrays(max_degree)\n",
    "    df,buckets = initialize_gain_buckets(df, buckets)\n",
    "    save_partition = copy.deepcopy(df['Partition'].values)\n",
    "    while(np.sum(df['Fixed']) < num_vertices):\n",
    "        if(len(df[df['Partition']==0]) >= len(df[df['Partition']==1])):\n",
    "            vertex_max_gain = calculate_vertex_max_gain(df,0,max_degree)\n",
    "            df, buckets = update_df_buckets(df,buckets,vertex_max_gain)           \n",
    "        else:\n",
    "            vertex_max_gain = calculate_vertex_max_gain(df,1,max_degree)\n",
    "            df, buckets = update_df_buckets(df,buckets,vertex_max_gain)\n",
    "            \n",
    "        num_cuts = calculate_num_cuts(df)\n",
    "        print(num_cuts)\n",
    "        if(num_cuts < min_cuts and len(df[df['Partition']==0]) == len(df[df['Partition']==1])):\n",
    "            save_partition = copy.deepcopy(df['Partition'].values)\n",
    "            min_cuts = num_cuts\n",
    "    return min_cuts, save_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "extraordinary-living",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "621\n",
      "611\n",
      "606\n",
      "599\n",
      "594\n",
      "588\n",
      "584\n",
      "578\n",
      "574\n",
      "564\n",
      "560\n",
      "554\n",
      "550\n",
      "553\n",
      "548\n",
      "543\n",
      "543\n",
      "538\n",
      "536\n",
      "534\n",
      "538\n",
      "533\n",
      "533\n",
      "528\n",
      "524\n",
      "520\n",
      "516\n",
      "514\n",
      "511\n",
      "507\n",
      "507\n",
      "509\n",
      "509\n",
      "508\n",
      "505\n",
      "513\n",
      "510\n",
      "510\n",
      "507\n",
      "503\n",
      "504\n",
      "504\n",
      "504\n",
      "504\n",
      "501\n",
      "497\n",
      "494\n",
      "494\n",
      "491\n",
      "491\n",
      "491\n",
      "495\n",
      "492\n",
      "492\n",
      "489\n",
      "489\n",
      "490\n",
      "490\n",
      "490\n",
      "491\n",
      "494\n",
      "495\n",
      "501\n",
      "502\n",
      "502\n",
      "499\n",
      "499\n",
      "500\n",
      "501\n",
      "504\n",
      "509\n",
      "510\n",
      "509\n",
      "506\n",
      "503\n",
      "503\n",
      "507\n",
      "512\n",
      "509\n",
      "509\n",
      "506\n",
      "506\n",
      "506\n",
      "506\n",
      "505\n",
      "506\n",
      "506\n",
      "507\n",
      "504\n",
      "501\n",
      "498\n",
      "495\n",
      "495\n",
      "499\n",
      "500\n",
      "497\n",
      "502\n",
      "499\n",
      "503\n",
      "500\n",
      "504\n",
      "505\n",
      "505\n",
      "504\n",
      "505\n",
      "506\n",
      "503\n",
      "503\n",
      "503\n",
      "502\n",
      "502\n",
      "505\n",
      "509\n",
      "509\n",
      "509\n",
      "509\n",
      "511\n",
      "512\n",
      "509\n",
      "510\n",
      "510\n",
      "511\n",
      "508\n",
      "508\n",
      "509\n",
      "514\n",
      "511\n",
      "512\n",
      "511\n",
      "508\n",
      "512\n",
      "513\n",
      "513\n",
      "513\n",
      "519\n",
      "519\n",
      "521\n",
      "521\n",
      "522\n",
      "522\n",
      "523\n",
      "524\n",
      "524\n",
      "525\n",
      "526\n",
      "526\n",
      "527\n",
      "524\n",
      "526\n",
      "535\n",
      "533\n",
      "537\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "538\n",
      "539\n",
      "537\n",
      "541\n",
      "543\n",
      "544\n",
      "544\n",
      "545\n",
      "546\n",
      "548\n",
      "546\n",
      "547\n",
      "548\n",
      "546\n",
      "547\n",
      "548\n",
      "546\n",
      "550\n",
      "552\n",
      "554\n",
      "556\n",
      "554\n",
      "554\n",
      "559\n",
      "561\n",
      "562\n",
      "560\n",
      "565\n",
      "567\n",
      "569\n",
      "570\n",
      "572\n",
      "569\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "568\n",
      "569\n",
      "567\n",
      "569\n",
      "570\n",
      "572\n",
      "574\n",
      "572\n",
      "573\n",
      "575\n",
      "577\n",
      "579\n",
      "577\n",
      "584\n",
      "586\n",
      "589\n",
      "591\n",
      "594\n",
      "596\n",
      "595\n",
      "596\n",
      "595\n",
      "596\n",
      "595\n",
      "596\n",
      "599\n",
      "597\n",
      "596\n",
      "598\n",
      "597\n",
      "606\n",
      "605\n",
      "607\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "612\n",
      "614\n",
      "617\n",
      "619\n",
      "618\n",
      "616\n",
      "622\n",
      "625\n",
      "624\n",
      "623\n",
      "624\n",
      "623\n",
      "622\n",
      "623\n",
      "626\n",
      "625\n",
      "624\n",
      "626\n",
      "629\n",
      "628\n",
      "627\n",
      "628\n",
      "627\n",
      "628\n",
      "631\n",
      "634\n",
      "633\n",
      "635\n",
      "631\n",
      "634\n",
      "634\n",
      "635\n",
      "647\n",
      "650\n",
      "652\n",
      "651\n",
      "657\n",
      "660\n",
      "660\n",
      "659\n",
      "662\n",
      "661\n",
      "661\n",
      "668\n",
      "668\n",
      "669\n",
      "673\n",
      "672\n",
      "672\n",
      "671\n",
      "671\n",
      "674\n",
      "677\n",
      "676\n",
      "676\n",
      "675\n",
      "677\n",
      "676\n",
      "678\n",
      "681\n",
      "685\n",
      "684\n",
      "688\n",
      "691\n",
      "691\n",
      "690\n",
      "690\n",
      "693\n",
      "693\n",
      "693\n",
      "692\n",
      "696\n",
      "696\n",
      "704\n",
      "704\n",
      "704\n",
      "704\n",
      "708\n",
      "712\n",
      "712\n",
      "712\n",
      "714\n",
      "715\n",
      "715\n",
      "720\n",
      "720\n",
      "719\n",
      "723\n",
      "724\n",
      "724\n",
      "725\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "728\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "744\n",
      "741\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "751\n",
      "752\n",
      "750\n",
      "751\n",
      "749\n",
      "750\n",
      "756\n",
      "757\n",
      "755\n",
      "755\n",
      "757\n",
      "758\n",
      "760\n",
      "761\n",
      "759\n",
      "760\n",
      "758\n",
      "764\n",
      "766\n",
      "768\n",
      "766\n",
      "766\n",
      "768\n",
      "766\n",
      "765\n",
      "767\n",
      "766\n",
      "768\n",
      "771\n",
      "773\n",
      "772\n",
      "774\n",
      "773\n",
      "775\n",
      "778\n",
      "780\n",
      "779\n",
      "777\n",
      "778\n",
      "780\n",
      "779\n",
      "781\n",
      "780\n",
      "782\n",
      "781\n",
      "784\n",
      "783\n",
      "782\n",
      "782\n",
      "781\n",
      "781\n",
      "780\n",
      "780\n",
      "779\n",
      "779\n",
      "778\n",
      "778\n",
      "777\n",
      "777\n",
      "776\n",
      "784\n",
      "783\n",
      "783\n",
      "782\n",
      "782\n",
      "781\n",
      "782\n",
      "781\n",
      "790\n",
      "789\n",
      "790\n",
      "790\n",
      "791\n",
      "791\n",
      "792\n",
      "792\n",
      "793\n",
      "793\n",
      "791\n",
      "791\n",
      "789\n",
      "785\n",
      "786\n",
      "787\n",
      "785\n",
      "786\n",
      "784\n",
      "785\n",
      "783\n",
      "784\n",
      "782\n",
      "780\n",
      "778\n",
      "776\n",
      "774\n",
      "772\n",
      "770\n",
      "768\n",
      "766\n",
      "768\n",
      "774\n",
      "772\n",
      "771\n",
      "769\n",
      "768\n",
      "766\n",
      "765\n",
      "763\n",
      "762\n",
      "760\n",
      "759\n",
      "757\n",
      "757\n",
      "755\n",
      "752\n",
      "750\n",
      "747\n",
      "746\n",
      "743\n",
      "742\n",
      "739\n",
      "739\n",
      "737\n",
      "738\n",
      "736\n",
      "733\n",
      "731\n",
      "728\n",
      "726\n",
      "723\n",
      "726\n",
      "724\n",
      "720\n",
      "718\n",
      "714\n",
      "713\n",
      "709\n",
      "705\n",
      "701\n",
      "697\n",
      "698\n",
      "694\n",
      "689\n",
      "686\n",
      "681\n",
      "678\n",
      "673\n",
      "670\n",
      "668\n",
      "663\n",
      "659\n",
      "654\n",
      "648\n",
      "643\n",
      "633\n",
      "627\n",
      "5.558052062988281\n"
     ]
    }
   ],
   "source": [
    "optimal_partition = None\n",
    "df = initialise_data(optimal_partition)\n",
    "\n",
    "current_num_cuts = calculate_num_cuts(df)\n",
    "start = time.time()\n",
    "local_optimum, optimal_partition = FM_one_pass(df)\n",
    "end = time.time()\n",
    "elpased_time = end - start\n",
    "print(elpased_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-nigeria",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-stupid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quantitative-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FM_one_run(max_passes):\n",
    "    flag = 0\n",
    "    total_passes = 0\n",
    "    total_time = 0\n",
    "    best_local_optimum = math.inf\n",
    "    optimal_partition = None\n",
    "    while(True):\n",
    "        df = initialise_data(optimal_partition)\n",
    "        current_num_cuts = calculate_num_cuts(df)\n",
    "        start = time.time()\n",
    "        local_optimum, optimal_partition = FM_one_pass(df)\n",
    "        total_passes += 1\n",
    "        end = time.time()\n",
    "        elpased_time = end - start\n",
    "        total_time += elpased_time\n",
    "\n",
    "        if(local_optimum < best_local_optimum and total_passes <= max_passes):\n",
    "            best_local_optimum = local_optimum\n",
    "            flag = 0\n",
    "        else:\n",
    "            flag = 1\n",
    "        if(flag == 1):\n",
    "            break\n",
    "    converged_local_optimum = best_local_optimum\n",
    "    return [converged_local_optimum, total_time, total_passes] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-overview",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "israeli-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FM_baseline\n",
    "max_passes = 10000\n",
    "total_runs = 25\n",
    "run_data_frame = pd.DataFrame(columns = ['Coverged local optima','Time(s)','Number of passes'])\n",
    "for i in range(0,25): \n",
    "    observations = FM_one_run(max_passes)\n",
    "    run_data_frame.loc[len(run_data_frame)] = observations\n",
    "#run_data_frame.to_csv(r'FM_baseline.csv', index = False)\n",
    "#np.savetxt(r'FM_baseline_median.txt', run_data_frame.median(), fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nets = []\n",
    "# for i in range(len(df)):\n",
    "#     conencted_vertices = df.loc[i]['ID connected vertices']\n",
    "#     for j in conencted_vertices:\n",
    "#         edge = (i,j)\n",
    "#         reverse_edge = (j,i)\n",
    "#         if(edge not in nets and reverse_edge not in nets):\n",
    "#             nets.append(edge)\n",
    "\n",
    "# nets\n",
    "\n",
    "# connected_nets_df = []\n",
    "# for i in range(num_vertices):\n",
    "#     connected_nets = []\n",
    "#     for j in nets:\n",
    "#         if i in j:\n",
    "#             connected_nets.append(j)\n",
    "#     connected_nets_df.append(connected_nets)\n",
    "\n",
    "# df['Nets'] = connected_nets_df\n",
    "\n",
    "# df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
